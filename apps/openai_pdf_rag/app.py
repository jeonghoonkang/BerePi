import os
import io
import subprocess
import time
import numpy as np
import pandas as pd
from fpdf import FPDF

import streamlit as st
from openai import BadRequestError, OpenAI
from PyPDF2 import PdfReader


MODEL_OPTIONS = {
    "gpt-3.5-turbo": "gpt-3.5-turbo",
    "gpt-4o-mini": "gpt-4o-mini",
    "llama-3": "meta-llama/Meta-Llama-3-8B-Instruct",
    "mistral": "mistralai/Mistral-7B-Instruct-v0.2",
    "gemma": "google/gemma-2b-it",
}


def load_hf_token() -> str | None:
    token = os.getenv("HF_TOKEN")
    if token:
        return token.strip()
    token_paths = [
        os.path.join(os.path.dirname(__file__), "hf_token.txt"),
        os.path.join(os.path.dirname(os.path.dirname(__file__)), "hf_token.txt"),
    ]
    for path in token_paths:
        if os.path.isfile(path):
            with open(path, "r", encoding="utf-8") as f:
                return f.read().strip()
    return None


def _verify_model_files(path: str) -> bool:
    config = os.path.join(path, "config.json")
    if not os.path.isfile(config) or os.path.getsize(config) <= 0:
        return False
    weight_files = [
        f
        for f in os.listdir(path)
        if f.endswith((".bin", ".safetensors"))
    ]
    if not weight_files:
        return False
    for wf in weight_files:
        if os.path.getsize(os.path.join(path, wf)) <= 0:
            return False
    return True


def ensure_model(repo_id: str) -> None:
    """Ensure that an OSS model is downloaded and valid before use."""
    if os.path.isdir(repo_id) and _verify_model_files(repo_id):
        return
    try:
        from huggingface_hub import hf_hub_download, snapshot_download, login
        from huggingface_hub.utils import GatedRepoError

    except Exception:
        st.error("huggingface_hub ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()

    token = load_hf_token()
    if token:
        try:
            login(token=token)
        except Exception:
            st.error("Hugging Face ë¡œê·¸ì¸ ì‹¤íŒ¨")
            st.stop()

    def _download() -> None:
        with st.spinner("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."):
            try:
                snapshot_download(
                    repo_id=repo_id,
                    local_dir=repo_id,
                    force_download=True,

                    token=token,
                )
            except GatedRepoError:
                st.error(
                    "í—ˆê¹…í˜ì´ìŠ¤ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤. HF_TOKEN í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” hf_token.txt íŒŒì¼ì„ ì„¤ì •í•˜ì„¸ìš”."
                )
                st.stop()
            except ValueError as e:
                st.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                st.stop()
            except Exception as e:
                st.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.stop()

        if _verify_model_files(repo_id):
            st.success("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        else:
            st.error("ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨")
            st.stop()

    try:
        hf_hub_download(
            repo_id=repo_id,
            filename="config.json",
            local_files_only=True,
            token=token,
        )
        if not _verify_model_files(repo_id):
            _download()
    except GatedRepoError:
        st.error(
            "í—ˆê¹…í˜ì´ìŠ¤ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤. HF_TOKEN í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” hf_token.txt íŒŒì¼ì„ ì„¤ì •í•˜ì„¸ìš”."
        )
        st.stop()
    except Exception:
        if st.button(f"{repo_id} ë‹¤ìš´ë¡œë“œ"):
            _download()

        else:
            st.stop()


def get_gpu_info() -> str:
    try:
        out = subprocess.check_output(
            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
            stderr=subprocess.DEVNULL,
        )
        gpus = [g for g in out.decode().strip().split("\n") if g]
        return f"{len(gpus)}ê°œ ({', '.join(gpus)})" if gpus else "GPU ì—†ìŒ"
    except Exception:
        return "GPU ì—†ìŒ"
      
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    key_candidates = [
        os.path.join(os.path.dirname(__file__), "nocommit_key.txt"),
        os.path.join(os.path.dirname(os.path.dirname(__file__)), "nocommit_key.txt"),
    ]
    for cand in key_candidates:
        if os.path.isfile(cand):
            try:
                with open(cand, "r", encoding="utf-8") as f:
                    OPENAI_API_KEY = f.read().strip()
                break
            except Exception:
                pass

client = OpenAI(api_key=OPENAI_API_KEY)

st.set_page_config(page_title="PDF RAG System")
st.title("ğŸ“„ PDF ê¸°ë°˜ Q&A")
st.write(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {get_gpu_info()}")
model_name = st.selectbox("ëª¨ë¸ ì„ íƒ", list(MODEL_OPTIONS.keys()))
model = MODEL_OPTIONS[model_name]
if model_name in ["llama-3", "mistral", "gemma"]:

    ensure_model(model)
st.write(f"ì‚¬ìš© ëª¨ë¸: {model_name}")

if "errors" not in st.session_state:
    st.session_state.errors = []


def log_error(msg: str) -> None:
    st.session_state.errors.append(msg)
    st.session_state.errors = st.session_state.errors[-3:]


def reset_app() -> None:
    for key in list(st.session_state.keys()):
        if key != "errors":
            del st.session_state[key]
    st.rerun()


if "docs" not in st.session_state:
    st.session_state.docs = None
    st.session_state.embs = None
    st.session_state.pdf_text = ""

if "history" not in st.session_state:
    st.session_state.history = []


def cache_paths(filename: str):
    base = os.path.splitext(filename)[0]
    return {
        "pdf": os.path.join(DATA_DIR, filename),
        "txt": os.path.join(DATA_DIR, base + ".txt"),
        "npz": os.path.join(DATA_DIR, base + ".npz"),
        "base": base,
    }


def load_cached_data():
    """Load cached PDFs and embeddings into session_state."""
    st.session_state.docs = []
    st.session_state.embs = []
    st.session_state.pdf_text = ""
    st.session_state.cached_files = []
    for fname in sorted(os.listdir(DATA_DIR)):
        if not fname.endswith(".npz"):
            continue
        base = os.path.splitext(fname)[0]
        paths = cache_paths(base + ".pdf")
        data = np.load(paths["npz"], allow_pickle=True)
        chunks = data["chunks"].tolist()
        embs = data["embeddings"]
        st.session_state.docs.extend(chunks)
        st.session_state.embs.extend(embs)
        txt_path = paths["txt"]
        if os.path.exists(txt_path):
            with open(txt_path, "r", encoding="utf-8") as f:
                st.session_state.pdf_text += f.read() + "\n"
        st.session_state.cached_files.append(base)


def save_pdf_and_embeddings(uploaded_file):
    paths = cache_paths(uploaded_file.name)
    if os.path.exists(paths["npz"]):
        return
    with open(paths["pdf"], "wb") as f:
        f.write(uploaded_file.getbuffer())
    text = read_pdf(paths["pdf"])
    with open(paths["txt"], "w", encoding="utf-8") as f:
        f.write(text)
    chunks = chunk_text(text)
    embs = []
    for chunk in chunks:
        resp = client.embeddings.create(model="text-embedding-3-small", input=[chunk])
        embs.append(np.array(resp.data[0].embedding))
    np.savez(paths["npz"], chunks=np.array(chunks, dtype=object), embeddings=np.array(embs))


def refresh_cache(selected_files):
    for name in selected_files:
        base = os.path.splitext(name)[0]
        paths = cache_paths(name)
        if os.path.exists(paths["pdf"]):
            text = read_pdf(paths["pdf"])
            with open(paths["txt"], "w", encoding="utf-8") as f:
                f.write(text)
            chunks = chunk_text(text)
            embs = []
            for chunk in chunks:
                resp = client.embeddings.create(
                    model="text-embedding-3-small", input=[chunk]
                )
                embs.append(np.array(resp.data[0].embedding))
            np.savez(
                paths["npz"],
                chunks=np.array(chunks, dtype=object),
                embeddings=np.array(embs),
            )


load_cached_data()


def read_pdf(file) -> str:
    text = ""
    reader = PdfReader(file)
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    return text


def chunk_text(text: str, chunk_size: int = 200, overlap: int = 50):
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        chunk = " ".join(words[start : start + chunk_size])
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks



uploaded_files = st.file_uploader(
    "PDF ì—…ë¡œë“œ", type="pdf", accept_multiple_files=True
)
mode = st.radio("ë‹µë³€ ëª¨ë“œ", ["ê¸°ë³¸", "PDF ì‚¬ìš©"])

if uploaded_files:
    st.subheader("ì—…ë¡œë“œëœ íŒŒì¼")
    total_size = sum(f.size for f in uploaded_files)
    st.write(f"ì´ {len(uploaded_files)}ê°œ íŒŒì¼, {total_size / 1024:.1f} KB")
    for uf in uploaded_files:
        st.write(f"- {uf.name} ({uf.size / 1024:.1f} KB)")

    texts = []
    all_chunks = []
    for uf in uploaded_files:
        text = read_pdf(uf)
        texts.append(text)
        all_chunks.extend(chunk_text(text))
    st.session_state.pdf_text = "\n\n".join(texts)
    st.session_state.docs = all_chunks
    st.session_state.embs = []
    with st.spinner("ì„ë² ë”© ìƒì„± ì¤‘..."):
        for chunk in st.session_state.docs:
            resp = client.embeddings.create(
                model="text-embedding-3-small",
                input=[chunk],
            )
            st.session_state.embs.append(np.array(resp.data[0].embedding))

    st.success("ë¬¸ì„œ ë¡œë”© ì™„ë£Œ")

if st.session_state.get("cached_files"):
    st.subheader("ìºì‹œëœ PDF")
    options = [f + ".pdf" for f in st.session_state.cached_files]
    selected = st.multiselect("ìºì‹œ íŒŒì¼ ì„ íƒ", options)
    c1, c2 = st.columns(2)
    with c1:
        if st.button("ì„ íƒ ì‚­ì œ") and selected:
            for name in selected:
                paths = cache_paths(name)
                for ext in ("pdf", "txt", "npz"):
                    p = paths[ext]
                    if os.path.exists(p):
                        os.remove(p)
            load_cached_data()
            st.success("ì‚­ì œ ì™„ë£Œ")
    with c2:
        if st.button("ì„ íƒ ìƒˆë¡œê³ ì¹¨") and selected:
            refresh_cache(selected)
            load_cached_data()
            st.success("ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ")

st.markdown("---")
if st.session_state.history:
    options = [f"{i+1}. {h['question']}" for i, h in enumerate(st.session_state.history)]
    sel = st.selectbox("ì´ì „ ì§ˆë¬¸ ì„ íƒ", [""] + options)
    if sel:
        idx = int(sel.split(".")[0]) - 1
        hist = st.session_state.history[idx]
        st.write(f"**ì§ˆë¬¸:** {hist['question']}")
        st.write(f"**ê¸°ë³¸ ë‹µë³€:** {hist['answer']}")
        if hist.get("pdf_answer"):
            st.write(f"**PDF ë‹µë³€:** {hist['pdf_answer']}")
        st.write(f"**ì‘ë‹µ ì‹œê°„:** {hist['elapsed']:.2f}ì´ˆ")

question = st.text_input("ì§ˆë¬¸ ì…ë ¥")

if question:
    default_box = st.container()
    pdf_box = st.container()
    text_box = st.container()

    with default_box:
        st.subheader("1. ê¸°ë³¸ ë‹µë³€")
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            start_t = time.perf_counter()
            try:
                resp = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": question}],
                )
            except BadRequestError as e:
                log_error(str(e))
                reset_app()

            elapsed_default = time.perf_counter() - start_t
            default_answer = resp.choices[0].message.content
            st.write(default_answer)
            st.write(f"â±ï¸ {elapsed_default:.2f}ì´ˆ")

    with pdf_box:
        st.subheader("2. PDF ê¸°ë°˜ ë‹µë³€")
        pdf_answer = None
        elapsed_pdf = None
        if mode == "PDF ì‚¬ìš©":
            if st.session_state.docs:
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    q_emb = np.array(
                        client.embeddings.create(
                            model="text-embedding-3-small", input=[question]
                        ).data[0].embedding
                    )
                    sims = [
                        float(
                            np.dot(q_emb, e)
                            / (np.linalg.norm(q_emb) * np.linalg.norm(e))
                        )
                        for e in st.session_state.embs
                    ]
                    top_indices = np.argsort(sims)[-3:][::-1]
                    context = "\n\n".join(
                        st.session_state.docs[i] for i in top_indices
                    )
                    prompt = (
                        "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n\në¬¸ì„œ ë‚´ìš©:\n"
                        + context
                        + "\n\nì§ˆë¬¸: "
                        + question
                    )
                    start_pdf = time.perf_counter()
                    try:
                        resp = client.chat.completions.create(
                            model=model,
                            messages=[{"role": "user", "content": prompt}],
                        )
                    except BadRequestError as e:
                        log_error(str(e))
                        reset_app()

                    elapsed_pdf = time.perf_counter() - start_pdf
                    pdf_answer = resp.choices[0].message.content
                    st.write(pdf_answer)
                    st.write(f"â±ï¸ {elapsed_pdf:.2f}ì´ˆ")

            else:
                st.warning("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    with text_box:
        st.subheader("3. PDF ë‚´ìš©")
        if st.session_state.pdf_text:
            st.text_area("", st.session_state.pdf_text, height=300)

    entry = {
        "question": question,
        "answer": default_answer,
        "elapsed": elapsed_default,
    }
    if pdf_answer:
        entry["pdf_answer"] = pdf_answer
        entry["pdf_elapsed"] = elapsed_pdf
    st.session_state.history.append(entry)
    if len(st.session_state.history) > 100:
        st.session_state.history.pop(0)

if st.session_state.history:
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ëª¨ë“  Q&A ì—‘ì…€ ì €ì¥"):
            df = pd.DataFrame(st.session_state.history)
            excel_io = io.BytesIO()
            df.to_excel(excel_io, index=False)
            st.download_button(
                "ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                excel_io.getvalue(),
                file_name="qa_history.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
    with col2:
        if st.button("ëª¨ë“  Q&A PDF ì €ì¥"):
            pdf = FPDF()
            pdf.add_page()
            pdf.set_font("Arial", size=12)
            for item in st.session_state.history:
                text = f"Q: {item['question']}\nA: {item['answer']}\n"
                if item.get('pdf_answer'):
                    text += f"PDF: {item['pdf_answer']}\n"
                pdf.multi_cell(0, 10, txt=text)
                pdf.ln()
            pdf_io = io.BytesIO(pdf.output(dest="S").encode("latin-1"))
            st.download_button(
                "PDF ë‹¤ìš´ë¡œë“œ",
                pdf_io.getvalue(),
                file_name="qa_history.pdf",
                mime="application/pdf",
            )

if st.session_state.errors:
    st.markdown("---")
    st.subheader("ì—ëŸ¬ ë©”ì‹œì§€")
    for err in st.session_state.errors[-3:]:
        st.error(err)
