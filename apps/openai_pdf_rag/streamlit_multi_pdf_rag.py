import subprocess
import os
from typing import List

import numpy as np
import streamlit as st
import ollama
from PyPDF2 import PdfReader


st.set_page_config(page_title="PDF RAG Chat")
st.title("ğŸ“„ PDF RAG Chat")

# í˜„ì¬ ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì™€ ì„ íƒ ë°•ìŠ¤ì— í‘œì‹œ
available_models = [m["name"] for m in ollama.list().get("models", [])]

# RAGì— ì‚¬ìš©í•  ìƒì„± ëª¨ë¸, ì„ë² ë”© ëª¨ë¸, ì¼ë°˜ QA ëª¨ë¸ì„ ê°ê° ì§€ì •
rag_model = (
    st.selectbox("RAG ìƒì„± ëª¨ë¸", options=available_models)
    if available_models
    else st.text_input("RAG ìƒì„± ëª¨ë¸ ì´ë¦„")
)
embed_models = [m for m in available_models if "embed" in m]
embed_model = (
    st.selectbox("ì„ë² ë”© ëª¨ë¸", options=embed_models)
    if embed_models
    else st.text_input("ì„ë² ë”© ëª¨ë¸ ì´ë¦„", "nomic-embed-text")
)
direct_model = (
    st.selectbox("ì¼ë°˜ QA ëª¨ë¸", options=available_models)
    if available_models
    else st.text_input("ì¼ë°˜ QA ëª¨ë¸ ì´ë¦„")
)

st.caption(f"RAG ëª¨ë¸: {rag_model} | ì¼ë°˜ ëª¨ë¸: {direct_model}")

st.markdown("### OSS ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
model_to_pull = st.text_input("ëª¨ë¸ ì´ë¦„ ì…ë ¥")
if st.button("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ") and model_to_pull:
    with st.spinner("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."):
        result = subprocess.run(
            ["ollama", "pull", model_to_pull], capture_output=True, text=True
        )
    if result.returncode == 0:
        st.success("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    else:
        st.error(result.stderr or "ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")


uploaded_files = st.file_uploader(
    "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf", accept_multiple_files=True
)

# ë³´ì—¬ì¤„ íŒŒì¼ ëª©ë¡
if uploaded_files:
    st.subheader("ì—…ë¡œë“œëœ íŒŒì¼")
    for f in uploaded_files:
        st.write(f"- {f.name}")


def read_pdf(file) -> str:
    text = ""
    reader = PdfReader(file)
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    return text


def split_text(text: str, chunk_size: int = 800, overlap: int = 200) -> List[str]:
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks


# ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ìª¼ê°œê³  ì„ë² ë”© ìƒì„±
all_chunks: List[str] = []
embeddings: List[np.ndarray] = []
if uploaded_files:
    for uf in uploaded_files:
        pdf_text = read_pdf(uf)
        all_chunks.extend(split_text(pdf_text))

    with st.spinner("ì„ë² ë”© ìƒì„± ì¤‘..."):
        for chunk in all_chunks:
            emb = ollama.embeddings(model=embed_model, prompt=chunk)["embedding"]

            embeddings.append(np.array(emb))
    st.success("ì„ë² ë”© ìƒì„± ì™„ë£Œ")

st.markdown("---")
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if question:
    # RAG ê¸°ë°˜ ë‹µë³€
    if embeddings:
        q_emb = ollama.embeddings(model=embed_model, prompt=question)["embedding"]
        q_emb = np.array(q_emb)

        sims = [
            float(np.dot(q_emb, e) / (np.linalg.norm(q_emb) * np.linalg.norm(e)))
            for e in embeddings
        ]
        top_indices = np.argsort(sims)[-3:][::-1]
        context = "\n\n".join(all_chunks[i] for i in top_indices)

        prompt = (
            "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n\n" + context + "\n\nì§ˆë¬¸: " + question
        )
        with st.spinner("RAG ë‹µë³€ ìƒì„± ì¤‘..."):
            completion = ollama.generate(model=rag_model, prompt=prompt)
            rag_answer = completion.get("response", "")
        st.text_area("RAG ë‹µë³€", rag_answer, height=200)
    else:
        st.warning("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    # ë¹„ RAG ì¼ë°˜ ë‹µë³€
    with st.spinner("ì¼ë°˜ ë‹µë³€ ìƒì„± ì¤‘..."):
        direct_completion = ollama.generate(model=direct_model, prompt=question)
        direct_answer = direct_completion.get("response", "")
    st.text_area("ì¼ë°˜ ëª¨ë¸ ë‹µë³€", direct_answer, height=200)

