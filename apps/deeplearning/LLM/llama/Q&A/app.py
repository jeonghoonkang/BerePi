import os
import time
import threading

import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline


def rerun() -> None:
    """Compatibility helper to rerun the Streamlit script."""
    if hasattr(st, "experimental_rerun"):
        st.experimental_rerun()
    else:
        st.rerun()


def display_gpu_status(tokenizer=None) -> None:
    """Display current GPU and model info at startup."""
    try:
        import torch

        # ì• í”Œ ì‹¤ë¦¬ì½˜ MPS ì§€ì› í™•ì¸
        if torch.backends.mps.is_available():
            st.info("Apple Silicon GPU (MPS) available")
            try:
                # MPS ë””ë°”ì´ìŠ¤ ì •ë³´ í‘œì‹œ
                device = torch.device("mps")
                st.info(f"Using device: {device}")
            except Exception as e:
                st.warning(f"MPS device error: {e}")
        elif torch.cuda.is_available():
            gpu_names = [f"{i}: {torch.cuda.get_device_name(i)}" for i in range(torch.cuda.device_count())]
            st.info("GPU available: " + ", ".join(gpu_names))

            mem_info = []
            for i in range(torch.cuda.device_count()):
                total = torch.cuda.get_device_properties(i).total_memory // (1024 ** 2)
                allocated = torch.cuda.memory_allocated(i) // (1024 ** 2)
                mem_info.append(f"{i}: {allocated}MB/{total}MB")
            st.info("GPU memory usage: " + ", ".join(mem_info))
        else:
            st.info("GPU not available, using CPU")
    except Exception as exc:  # pragma: no cover - GPU inspection can fail
        st.warning(f"Could not determine GPU status: {exc}")

    if tokenizer is not None:
        try:
            st.info(f"Max input tokens: {getattr(tokenizer, 'model_max_length', 'unknown')}")
        except Exception:  # pragma: no cover - tokenizer may be malformed
            pass

st.set_page_config(page_title="Llama Q&A", page_icon="ğŸ¦™")

st.title("ğŸ¦™ Llama ê¸°ë°˜ Q&A ë°ëª¨")

# ì‚¬ì´ë“œë°”ì— ëª¨ë¸ ì„ íƒ ì˜µì…˜ ì¶”ê°€
st.sidebar.header("ëª¨ë¸ ì„¤ì •")
model_option = st.sidebar.selectbox(
    "ëª¨ë¸ ì„ íƒ",
    [
        "NousResearch/Llama-2-7b-chat-hf",
        "NousResearch/Llama-2-13b-chat-hf",
        "microsoft/DialoGPT-medium",
        "gpt2",
        "distilgpt2"
    ],
    index=0
)

# ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
st.sidebar.header("ìƒì„± íŒŒë¼ë¯¸í„°")
max_length = st.sidebar.slider("ìµœëŒ€ ê¸¸ì´", 100, 2048, 512)
temperature = st.sidebar.slider("Temperature", 0.1, 2.0, 0.7)
top_p = st.sidebar.slider("Top-p", 0.1, 1.0, 0.9)
do_sample = st.sidebar.checkbox("ìƒ˜í”Œë§ ì‚¬ìš©", value=True)


def download_model(model_name: str) -> None:
    """Download the model showing a simple progress bar."""
    try:
        from huggingface_hub import snapshot_download
    except Exception:
        st.error("huggingface_hub is required to download models")
        st.stop()

    progress = st.progress(0)

    def _download():
        try:
            snapshot_download(repo_id=model_name, local_dir=model_name, resume_download=True)
        except Exception as exc:
            st.session_state.download_error = str(exc)

    thread = threading.Thread(target=_download)
    thread.start()
    pct = 0
    while thread.is_alive():
        pct = min(100, pct + 1)
        progress.progress(pct / 100.0)
        time.sleep(1)

    thread.join()
    progress.progress(1.0)
    if "download_error" in st.session_state:
        st.error("Failed to download model: " + st.session_state.download_error)
        st.stop()
    st.success("Download complete")


def ensure_model(model_name: str) -> None:
    """Ensure the model files are available. Auto download after 10s."""
    if os.path.isdir(model_name):
        return
    try:
        from huggingface_hub import hf_hub_download

        # check config and at least one weight file exists locally
        hf_hub_download(repo_id=model_name, filename="config.json", local_files_only=True)
        try:
            hf_hub_download(repo_id=model_name, filename="pytorch_model.bin", local_files_only=True)
        except Exception:
            hf_hub_download(repo_id=model_name, filename="pytorch_model.bin.index.json", local_files_only=True)
        return
    except Exception:
        pass

    if "prompt_time" not in st.session_state:
        st.session_state.prompt_time = time.time()

    elapsed = time.time() - st.session_state.prompt_time
    remaining = int(10 - elapsed)

    if "download_decision" not in st.session_state:
        st.session_state.download_decision = None

    if st.session_state.download_decision is None:
        st.warning(f"Model '{model_name}' not found. Download? (auto in {max(remaining,0)}s)")
        cols = st.columns(2)
        if cols[0].button("Download now"):
            st.session_state.download_decision = True
            rerun()

        if cols[1].button("Cancel"):
            st.session_state.download_decision = False
            st.stop()
        if remaining <= 0:
            st.session_state.download_decision = True
            rerun()
        else:
            time.sleep(1)
            rerun()

    elif st.session_state.download_decision:
        download_model(model_name)
    else:
        st.stop()


# ëª¨ë¸ ë¡œë”©
MODEL_NAME = model_option
ensure_model(MODEL_NAME)


@st.cache_resource
def load_model(name: str):
    import torch
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œ trust_remote_code=True ì¶”ê°€
    tokenizer = AutoTokenizer.from_pretrained(name, trust_remote_code=True)
    
    # íŒ¨ë”© í† í°ì´ ì—†ìœ¼ë©´ EOS í† í°ì„ íŒ¨ë”© í† í°ìœ¼ë¡œ ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    try:
        # ìš°ì„  ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©í•˜ì—¬ ë¡œë“œ ì‹œë„
        model = AutoModelForCausalLM.from_pretrained(
            name,
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True,
        )
    except FileNotFoundError:
        # ë¡œì»¬ íŒŒì¼ì´ ëˆ„ë½ëœ ê²½ìš° ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
        st.warning("ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ ë‹¤ìš´ë¡œë“œë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
        with st.spinner("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."):
            model = AutoModelForCausalLM.from_pretrained(
                name,
                device_map="auto",
                trust_remote_code=True,
            )
    except Exception:
        # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” ë‹¤ì‹œ ì‹œë„í•˜ì—¬ ì²˜ë¦¬
        with st.spinner("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."):
            model = AutoModelForCausalLM.from_pretrained(
                name,
                device_map="auto",
                trust_remote_code=True,
            )

    generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

    return generator


# ëª¨ë¸ ë¡œë“œ
with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘..."):
    generator = load_model(MODEL_NAME)
    display_gpu_status(generator.tokenizer)

# ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… í•¨ìˆ˜
def format_prompt(user_input: str, model_name: str) -> str:
    """ëª¨ë¸ë³„ ì±„íŒ… í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
    if "llama" in model_name.lower():
        return f"""<s>[INST] {user_input} [/INST]"""
    elif "dialogpt" in model_name.lower():
        return f"Human: {user_input}\nAssistant:"
    else:
        return user_input

# ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
st.header("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        time_placeholder = st.empty()
        
        try:
            start_time = time.time()
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                # ëª¨ë¸ë³„ ì±„íŒ… í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
                formatted_prompt = format_prompt(prompt, MODEL_NAME)
                
                response = generator(
                    formatted_prompt, 
                    max_length=max_length, 
                    do_sample=do_sample,
                    temperature=temperature,
                    top_p=top_p,
                    pad_token_id=generator.tokenizer.eos_token_id
                )
                
                # ì‘ë‹µì—ì„œ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°í•˜ê³  ë‹µë³€ë§Œ ì¶”ì¶œ
                full_response = response[0]["generated_text"]
                answer = full_response[len(formatted_prompt):].strip()
                
                # ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í‘œì‹œ
                full_response_text = ""
                for chunk in answer.split():
                    full_response_text += chunk + " "
                    message_placeholder.markdown(full_response_text + "â–Œ")
                    time.sleep(0.05)
                message_placeholder.markdown(full_response_text)
            elapsed = time.time() - start_time
            time_placeholder.markdown(f"_(ì‘ë‹µ ì‹œê°„: {elapsed:.2f}ì´ˆ)_")
                
        except Exception as exc:
            message_placeholder.error("ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            with st.expander("ì˜¤ë¥˜ ìƒì„¸ ë³´ê¸°"):
                st.exception(exc)
        else:
            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "assistant", "content": answer})

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ë²„íŠ¼
if st.sidebar.button("ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.rerun()

# ëª¨ë¸ ì •ë³´ í‘œì‹œ
st.sidebar.header("ëª¨ë¸ ì •ë³´")
st.sidebar.info(f"í˜„ì¬ ëª¨ë¸: {MODEL_NAME}")
st.sidebar.info(f"í† í¬ë‚˜ì´ì €: {generator.tokenizer.__class__.__name__}")

# ì‚¬ìš©ë²• ì•ˆë‚´
with st.expander("ì‚¬ìš©ë²•"):
    st.markdown("""
    ### Llama Q&A ì‚¬ìš©ë²•
    
    1. **ëª¨ë¸ ì„ íƒ**: ì‚¬ì´ë“œë°”ì—ì„œ ì›í•˜ëŠ” Llama ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”
    2. **íŒŒë¼ë¯¸í„° ì¡°ì •**: Temperature, Top-p ë“±ì„ ì¡°ì •í•˜ì—¬ ë‹µë³€ì˜ ì°½ì˜ì„±ì„ ì¡°ì ˆí•˜ì„¸ìš”
    3. **ì§ˆë¬¸ ì…ë ¥**: ì±„íŒ…ì°½ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”
    4. **ë‹µë³€ í™•ì¸**: Llamaê°€ ìƒì„±í•œ ë‹µë³€ì„ í™•ì¸í•˜ì„¸ìš”
    
    ### íŒ
    - ë” í° ëª¨ë¸(13B, 70B)ì€ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ë§Œ ë¡œë”© ì‹œê°„ì´ ê¸¸ì–´ì§‘ë‹ˆë‹¤
    - Temperatureë¥¼ ë‚®ì¶”ë©´ ë” ì¼ê´€ëœ ë‹µë³€ì„, ë†’ì´ë©´ ë” ì°½ì˜ì ì¸ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - Apple Silicon Macì—ì„œëŠ” MPS ê°€ì†ì„ ìë™ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤
    """) 